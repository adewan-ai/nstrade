{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>id</th>\n",
       "      <th>return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>118053</th>\n",
       "      <td>2025-05-16 21:00:00</td>\n",
       "      <td>103643.59</td>\n",
       "      <td>3.696248e+07</td>\n",
       "      <td>118053</td>\n",
       "      <td>-0.000890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118054</th>\n",
       "      <td>2025-05-16 22:00:00</td>\n",
       "      <td>103551.32</td>\n",
       "      <td>1.012609e+08</td>\n",
       "      <td>118054</td>\n",
       "      <td>-0.000499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118055</th>\n",
       "      <td>2025-05-16 23:00:00</td>\n",
       "      <td>103499.60</td>\n",
       "      <td>3.235507e+07</td>\n",
       "      <td>118055</td>\n",
       "      <td>-0.001078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118056</th>\n",
       "      <td>2025-05-17 00:00:00</td>\n",
       "      <td>103388.02</td>\n",
       "      <td>5.044067e+07</td>\n",
       "      <td>118056</td>\n",
       "      <td>-0.005115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118057</th>\n",
       "      <td>2025-05-17 01:00:00</td>\n",
       "      <td>102859.21</td>\n",
       "      <td>6.891526e+07</td>\n",
       "      <td>118057</td>\n",
       "      <td>0.004354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 timestamp      close        volume      id    return\n",
       "118053 2025-05-16 21:00:00  103643.59  3.696248e+07  118053 -0.000890\n",
       "118054 2025-05-16 22:00:00  103551.32  1.012609e+08  118054 -0.000499\n",
       "118055 2025-05-16 23:00:00  103499.60  3.235507e+07  118055 -0.001078\n",
       "118056 2025-05-17 00:00:00  103388.02  5.044067e+07  118056 -0.005115\n",
       "118057 2025-05-17 01:00:00  102859.21  6.891526e+07  118057  0.004354"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_for_model(data_path):\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "    df = df[['time', 'close', 'volumeto']].copy()\n",
    "    df = df.rename(columns={'volumeto': 'volume'})\n",
    "    df['id'] = 'BTC'\n",
    "    df['time'] = pd.to_datetime(df['time']).dt.tz_localize(None)\n",
    "    df['return'] = df['close'].pct_change().shift(-1)  # 1-step-ahead return\n",
    "    df=df.dropna(subset=['return'])\n",
    "    df.info()\n",
    "    df.head()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoMLStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from autogluon.timeseries import TimeSeriesDataFrame, TimeSeriesPredictor\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "from strategy import Strategy\n",
    "from backtest import run_backtest\n",
    "\n",
    "class AutoMLStrategy(Strategy):\n",
    "    def __init__(self, initial_capital=10000, df=None, lookback=1000):\n",
    "        super().__init__(initial_capital)\n",
    "        self.has_bought = False\n",
    "        self.lookback = lookback           # how many bars to keep\n",
    "        self.window   = pd.DataFrame()  # seed with training tail\n",
    "\n",
    "        train_df = TimeSeriesDataFrame.from_data_frame(\n",
    "            df,          # drop last NaN caused by shift\n",
    "            timestamp_column='time',\n",
    "            id_column='id',\n",
    "            static_features_df=None\n",
    "        )\n",
    "        self.model = TimeSeriesPredictor(\n",
    "            target='return',\n",
    "            prediction_length=1,\n",
    "            eval_metric='rmse',\n",
    "            verbosity=4,\n",
    "            quantile_levels=[0.05, 0.25, 0.5, 0.75, 0.95],  # gives you a distribution\n",
    "            freq='1H'             # or '1H', '5min', …  match your data\n",
    "            # presets='medium_quality'\n",
    "        )\n",
    "        self.model.fit(train_df)\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # called once per new bar by the back-testing engine\n",
    "    # -----------------------------------------------------------------\n",
    "    def process_bar(self, bar):\n",
    "        \"\"\"\n",
    "        Append the new bar to our rolling window and feed the\n",
    "        *window* to AutoGluon, not just the latest row.\n",
    "        \"\"\"\n",
    "        # 1) Keep the last `lookback` rows:\n",
    "        self.window = (\n",
    "            pd.concat([self.window, pd.DataFrame([bar])])\n",
    "              .groupby('id')\n",
    "              .tail(self.lookback)\n",
    "              .reset_index(drop=True)\n",
    "        )\n",
    "        print(self.window)\n",
    "\n",
    "        # 2) Convert to TimeSeriesDataFrame\n",
    "        latest_tsd = TimeSeriesDataFrame.from_data_frame(\n",
    "            self.window,\n",
    "            timestamp_column='time',\n",
    "            id_column='id',\n",
    "        )\n",
    "\n",
    "        # 3) Forecast 1-step-ahead return distribution\n",
    "        self.fcst = self.model.predict(latest_tsd).iloc[0]\n",
    "\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # turn forecast + risk estimate into trading instruction\n",
    "    # -----------------------------------------------------------------\n",
    "    def get_signal(self) -> str:\n",
    "        \"\"\"\n",
    "        BUY  :  μ > k·σ  and entire 90 % interval is positive\n",
    "        SELL :  μ < −k·σ and entire 90 % interval is negative\n",
    "        HOLD :  otherwise\n",
    "        \"\"\"\n",
    "\n",
    "        # Need enough history to compute σ -- otherwise stay flat\n",
    "        if len(self.returns_hist) < self.lookback:\n",
    "            return \"hold\"\n",
    "\n",
    "        sigma  = np.std(self.returns_hist)\n",
    "        k_sigma = self.thresh_factor * sigma\n",
    "\n",
    "        mu   = self.fcst[\"0.5\"]   # median / mean\n",
    "        q05  = self.fcst[\"0.05\"]\n",
    "        q95  = self.fcst[\"0.95\"]\n",
    "\n",
    "        # --- decision logic ------------------------------------------\n",
    "        if (mu > k_sigma) and (q05 > 0) and not self.has_position:\n",
    "            self.has_position = True\n",
    "            return \"buy\"\n",
    "\n",
    "        if (mu < -k_sigma) and (q95 < 0) and self.has_position:\n",
    "            self.has_position = False\n",
    "            return \"sell\"\n",
    "\n",
    "        return \"hold\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiro/play/nstrade/.venv/lib/python3.12/site-packages/autogluon/timeseries/predictor.py:198: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  offset = pd.tseries.frequencies.to_offset(self.freq)\n",
      "Frequency '1H' stored as 'h'\n",
      "Beginning AutoGluon training...\n",
      "AutoGluon will save models to '/Users/jiro/play/nstrade/notebooks/AutogluonModels/ag-20250517_083050'\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.0\n",
      "Python Version:     3.12.8\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 23.6.0: Thu Mar  6 22:08:50 PST 2025; root:xnu-10063.141.1.704.6~1/RELEASE_ARM64_T8112\n",
      "CPU Count:          8\n",
      "GPU Count:          0\n",
      "Memory Avail:       5.68 GB / 16.00 GB (35.5%)\n",
      "Disk Space Avail:   5.63 GB / 228.27 GB (2.5%)\n",
      "\tWARNING: Available disk space is low and there is a risk that AutoGluon will run out of disk during fit, causing an exception. \n",
      "\tWe recommend a minimum available disk space of 10 GB, and large datasets may require more.\n",
      "===================================================\n",
      "\n",
      "Fitting with arguments:\n",
      "{'enable_ensemble': True,\n",
      " 'eval_metric': RMSE,\n",
      " 'freq': 'h',\n",
      " 'hyperparameters': 'default',\n",
      " 'known_covariates_names': [],\n",
      " 'num_val_windows': 1,\n",
      " 'prediction_length': 1,\n",
      " 'quantile_levels': [0.05, 0.25, 0.5, 0.75, 0.95],\n",
      " 'random_seed': 123,\n",
      " 'refit_every_n_windows': 1,\n",
      " 'refit_full': False,\n",
      " 'skip_model_selection': False,\n",
      " 'target': 'return',\n",
      " 'verbosity': 4}\n",
      "\n",
      "Provided train_data has 2000 rows, 1 time series. Median time series length is 2000 (min=2000, max=2000). \n",
      "\n",
      "Provided data contains following columns:\n",
      "\ttarget: 'return'\n",
      "\tpast_covariates:\n",
      "\t\tcategorical:        []\n",
      "\t\tcontinuous (float): ['close', 'volume']\n",
      "\n",
      "To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit\n",
      "\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'RMSE'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "===================================================\n",
      "\n",
      "Starting training. Start time is 2025-05-17 16:30:50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 118058 entries, 0 to 118057\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count   Dtype         \n",
      "---  ------  --------------   -----         \n",
      " 0   time    118058 non-null  datetime64[ns]\n",
      " 1   close   118058 non-null  float64       \n",
      " 2   volume  118058 non-null  float64       \n",
      " 3   id      118058 non-null  object        \n",
      " 4   return  118058 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(3), object(1)\n",
      "memory usage: 5.4+ MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Models that will be trained: ['SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'NPTS', 'DynamicOptimizedTheta', 'AutoETS', 'ChronosZeroShot[bolt_base]', 'ChronosFineTuned[bolt_small]', 'TemporalFusionTransformer', 'DeepAR', 'PatchTST', 'TiDE']\n",
      "Training timeseries model SeasonalNaive. \n",
      "\tWindow 0\n",
      "Shortening all time series to at most 2500\n",
      "\t\t-0.0033      = Validation score (-RMSE)\n",
      "\t\t0.002   s    = Training runtime\n",
      "\t\t1.141   s    = Prediction runtime\n",
      "\t-0.0033       = Validation score (-RMSE)\n",
      "\t0.01    s     = Training runtime\n",
      "\t1.14    s     = Validation (prediction) runtime\n",
      "Training timeseries model RecursiveTabular. \n",
      "\tWindow 0\n",
      "Shortening all series to at most 1000025\n",
      "train_df shape: (1974, 47), val_df shape: (1, 47)\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.0\n",
      "Python Version:     3.12.8\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 23.6.0: Thu Mar  6 22:08:50 PST 2025; root:xnu-10063.141.1.704.6~1/RELEASE_ARM64_T8112\n",
      "CPU Count:          8\n",
      "Memory Avail:       5.79 GB / 16.00 GB (36.2%)\n",
      "Disk Space Avail:   5.62 GB / 228.27 GB (2.5%)\n",
      "\tWARNING: Available disk space is low and there is a risk that AutoGluon will run out of disk during fit, causing an exception. \n",
      "\tWe recommend a minimum available disk space of 10 GB, and large datasets may require more.\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='experimental' : New in v1.2: Pre-trained foundation model + parallel fits. The absolute best accuracy without consideration for inference speed. Does not support GPU.\n",
      "\tpresets='best'         : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'         : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'         : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'       : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"/Users/jiro/play/nstrade/notebooks/AutogluonModels/ag-20250517_083050/models/RecursiveTabular/W0/tabular_predictor\"\n",
      "Train Data Rows:    1974\n",
      "Train Data Columns: 44\n",
      "Tuning Data Rows:    1\n",
      "Tuning Data Columns: 44\n",
      "Label Column:       y\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    5919.49 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.33 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 44 | ['lag1', 'lag2', 'lag3', 'lag4', 'lag5', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 44 | ['lag1', 'lag2', 'lag3', 'lag4', 'lag5', ...]\n",
      "\t0.0s = Fit runtime\n",
      "\t44 features in original data used to generate 44 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.03s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'GBM': [{}],\n",
      "}\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM ...\n",
      "\t-0.009600000455975533\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.54s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'LightGBM': 1.0}\n",
      "\t-0.009600000455975533\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 0.58s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 822.6 rows/s (1 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/jiro/play/nstrade/notebooks/AutogluonModels/ag-20250517_083050/models/RecursiveTabular/W0/tabular_predictor\")\n",
      "Shortening all series to at most 1000025\n",
      "\t\t-0.0052      = Validation score (-RMSE)\n",
      "\t\t0.687   s    = Training runtime\n",
      "\t\t0.014   s    = Prediction runtime\n",
      "\t-0.0052       = Validation score (-RMSE)\n",
      "\t0.69    s     = Training runtime\n",
      "\t0.01    s     = Validation (prediction) runtime\n",
      "Training timeseries model DirectTabular. \n",
      "\tWindow 0\n",
      "Shortening all series to at most 1000001\n",
      "train_df shape: (1998, 47), val_df shape: (1, 47)\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.0\n",
      "Python Version:     3.12.8\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 23.6.0: Thu Mar  6 22:08:50 PST 2025; root:xnu-10063.141.1.704.6~1/RELEASE_ARM64_T8112\n",
      "CPU Count:          8\n",
      "Memory Avail:       5.75 GB / 16.00 GB (35.9%)\n",
      "Disk Space Avail:   5.62 GB / 228.27 GB (2.5%)\n",
      "\tWARNING: Available disk space is low and there is a risk that AutoGluon will run out of disk during fit, causing an exception. \n",
      "\tWe recommend a minimum available disk space of 10 GB, and large datasets may require more.\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='experimental' : New in v1.2: Pre-trained foundation model + parallel fits. The absolute best accuracy without consideration for inference speed. Does not support GPU.\n",
      "\tpresets='best'         : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'         : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'         : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'       : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"/Users/jiro/play/nstrade/notebooks/AutogluonModels/ag-20250517_083050/models/DirectTabular/W0/tabular_predictor\"\n",
      "Train Data Rows:    1998\n",
      "Train Data Columns: 44\n",
      "Tuning Data Rows:    1\n",
      "Tuning Data Columns: 44\n",
      "Label Column:       y\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    5887.95 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 44 | ['lag1', 'lag2', 'lag3', 'lag4', 'lag5', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 44 | ['lag1', 'lag2', 'lag3', 'lag4', 'lag5', ...]\n",
      "\t0.0s = Fit runtime\n",
      "\t44 features in original data used to generate 44 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.34 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.03s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'GBM': [{}],\n",
      "}\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM ...\n",
      "\t-0.00039999998989515007\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.87s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'LightGBM': 1.0}\n",
      "\t-0.00039999998989515007\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 0.92s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 1395.3 rows/s (1 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/jiro/play/nstrade/notebooks/AutogluonModels/ag-20250517_083050/models/DirectTabular/W0/tabular_predictor\")\n",
      "Shortening all series to at most 1000002\n",
      "Shortening all series to at most 1000001\n",
      "\t\t-0.0026      = Validation score (-RMSE)\n",
      "\t\t0.954   s    = Training runtime\n",
      "\t\t0.034   s    = Prediction runtime\n",
      "\t-0.0026       = Validation score (-RMSE)\n",
      "\t0.96    s     = Training runtime\n",
      "\t0.03    s     = Validation (prediction) runtime\n",
      "Training timeseries model NPTS. \n",
      "\tWindow 0\n",
      "Shortening all time series to at most 2500\n",
      "\t\t-0.0041      = Validation score (-RMSE)\n",
      "\t\t0.001   s    = Training runtime\n",
      "\t\t0.657   s    = Prediction runtime\n",
      "\t-0.0041       = Validation score (-RMSE)\n",
      "\t0.01    s     = Training runtime\n",
      "\t0.66    s     = Validation (prediction) runtime\n",
      "Training timeseries model DynamicOptimizedTheta. \n",
      "\tWindow 0\n",
      "Shortening all time series to at most 2500\n",
      "\t\t-0.0060      = Validation score (-RMSE)\n",
      "\t\t0.002   s    = Training runtime\n",
      "\t\t11.674  s    = Prediction runtime\n",
      "\t-0.0060       = Validation score (-RMSE)\n",
      "\t0.01    s     = Training runtime\n",
      "\t11.67   s     = Validation (prediction) runtime\n",
      "Training timeseries model AutoETS. \n",
      "\tWindow 0\n",
      "Shortening all time series to at most 2500\n",
      "\t\t-0.0044      = Validation score (-RMSE)\n",
      "\t\t0.001   s    = Training runtime\n",
      "\t\t0.487   s    = Prediction runtime\n",
      "\t-0.0044       = Validation score (-RMSE)\n",
      "\t0.01    s     = Training runtime\n",
      "\t0.49    s     = Validation (prediction) runtime\n",
      "Training timeseries model ChronosZeroShot[bolt_base]. \n",
      "\tWindow 0\n",
      "\tWarning: Exception caused ChronosZeroShot[bolt_base] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_tf_utils because of the following error (look up to see its traceback):\n",
      "Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jiro/play/nstrade/.venv/lib/python3.12/site-packages/transformers/activations_tf.py\", line 22, in <module>\n",
      "    import tf_keras as keras\n",
      "ModuleNotFoundError: No module named 'tf_keras'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jiro/play/nstrade/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py\", line 1863, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 999, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"/Users/jiro/play/nstrade/.venv/lib/python3.12/site-packages/transformers/modeling_tf_utils.py\", line 38, in <module>\n",
      "    from .activations_tf import get_tf_activation\n",
      "  File \"/Users/jiro/play/nstrade/.venv/lib/python3.12/site-packages/transformers/activations_tf.py\", line 27, in <module>\n",
      "    raise ValueError(\n",
      "ValueError: Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jiro/play/nstrade/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py\", line 1863, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 999, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"/Users/jiro/play/nstrade/.venv/lib/python3.12/site-packages/transformers/integrations/integration_utils.py\", line 36, in <module>\n",
      "    from .. import PreTrainedModel, TFPreTrainedModel\n",
      "  File \"<frozen importlib._bootstrap>\", line 1412, in _handle_fromlist\n",
      "  File \"/Users/jiro/play/nstrade/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py\", line 1851, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jiro/play/nstrade/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py\", line 1865, in _get_module\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Failed to import transformers.modeling_tf_utils because of the following error (look up to see its traceback):\n",
      "Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jiro/play/nstrade/.venv/lib/python3.12/site-packages/autogluon/timeseries/trainer.py\", line 357, in _train_and_save\n",
      "    model = self._train_single(train_data, model, val_data=val_data, time_limit=time_limit)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jiro/play/nstrade/.venv/lib/python3.12/site-packages/autogluon/timeseries/trainer.py\", line 273, in _train_single\n",
      "    model.fit(\n",
      "  File \"/Users/jiro/play/nstrade/.venv/lib/python3.12/site-packages/autogluon/timeseries/models/abstract/abstract_timeseries_model.py\", line 511, in fit\n",
      "    self._fit(\n",
      "  File \"/Users/jiro/play/nstrade/.venv/lib/python3.12/site-packages/autogluon/timeseries/models/multi_window/multi_window_model.py\", line 137, in _fit\n",
      "    model.fit(\n",
      "  File \"/Users/jiro/play/nstrade/.venv/lib/python3.12/site-packages/autogluon/timeseries/models/abstract/abstract_timeseries_model.py\", line 511, in fit\n",
      "    self._fit(\n",
      "  File \"/Users/jiro/play/nstrade/.venv/lib/python3.12/site-packages/autogluon/timeseries/models/chronos/model.py\", line 422, in _fit\n",
      "    from transformers.trainer import PrinterCallback, Trainer, TrainingArguments\n",
      "  File \"/Users/jiro/play/nstrade/.venv/lib/python3.12/site-packages/transformers/trainer.py\", line 42, in <module>\n",
      "    from .integrations import (\n",
      "  File \"<frozen importlib._bootstrap>\", line 1412, in _handle_fromlist\n",
      "  File \"/Users/jiro/play/nstrade/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py\", line 1851, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jiro/play/nstrade/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py\", line 1865, in _get_module\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Failed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_tf_utils because of the following error (look up to see its traceback):\n",
      "Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.\n",
      "\n",
      "Training timeseries model ChronosFineTuned[bolt_small]. \n",
      "\tWindow 0\n",
      "\tSkipping covariate_regressor since the dataset contains no covariates or static features.\n",
      "\tWarning: Exception caused ChronosFineTuned[bolt_small] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_tf_utils because of the following error (look up to see its traceback):\n",
      "Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jiro/play/nstrade/.venv/lib/python3.12/site-packages/transformers/activations_tf.py\", line 22, in <module>\n",
      "    import tf_keras as keras\n",
      "ModuleNotFoundError: No module named 'tf_keras'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jiro/play/nstrade/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py\", line 1863, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 999, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"/Users/jiro/play/nstrade/.venv/lib/python3.12/site-packages/transformers/modeling_tf_utils.py\", line 38, in <module>\n",
      "    from .activations_tf import get_tf_activation\n",
      "  File \"/Users/jiro/play/nstrade/.venv/lib/python3.12/site-packages/transformers/activations_tf.py\", line 27, in <module>\n",
      "    raise ValueError(\n",
      "ValueError: Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jiro/play/nstrade/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py\", line 1863, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 999, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"/Users/jiro/play/nstrade/.venv/lib/python3.12/site-packages/transformers/integrations/integration_utils.py\", line 36, in <module>\n",
      "    from .. import PreTrainedModel, TFPreTrainedModel\n",
      "  File \"<frozen importlib._bootstrap>\", line 1412, in _handle_fromlist\n",
      "  File \"/Users/jiro/play/nstrade/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py\", line 1851, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jiro/play/nstrade/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py\", line 1865, in _get_module\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Failed to import transformers.modeling_tf_utils because of the following error (look up to see its traceback):\n",
      "Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jiro/play/nstrade/.venv/lib/python3.12/site-packages/autogluon/timeseries/trainer.py\", line 357, in _train_and_save\n",
      "    model = self._train_single(train_data, model, val_data=val_data, time_limit=time_limit)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jiro/play/nstrade/.venv/lib/python3.12/site-packages/autogluon/timeseries/trainer.py\", line 273, in _train_single\n",
      "    model.fit(\n",
      "  File \"/Users/jiro/play/nstrade/.venv/lib/python3.12/site-packages/autogluon/timeseries/models/abstract/abstract_timeseries_model.py\", line 511, in fit\n",
      "    self._fit(\n",
      "  File \"/Users/jiro/play/nstrade/.venv/lib/python3.12/site-packages/autogluon/timeseries/models/multi_window/multi_window_model.py\", line 137, in _fit\n",
      "    model.fit(\n",
      "  File \"/Users/jiro/play/nstrade/.venv/lib/python3.12/site-packages/autogluon/timeseries/models/abstract/abstract_timeseries_model.py\", line 511, in fit\n",
      "    self._fit(\n",
      "  File \"/Users/jiro/play/nstrade/.venv/lib/python3.12/site-packages/autogluon/timeseries/models/chronos/model.py\", line 422, in _fit\n",
      "    from transformers.trainer import PrinterCallback, Trainer, TrainingArguments\n",
      "  File \"/Users/jiro/play/nstrade/.venv/lib/python3.12/site-packages/transformers/trainer.py\", line 42, in <module>\n",
      "    from .integrations import (\n",
      "  File \"<frozen importlib._bootstrap>\", line 1412, in _handle_fromlist\n",
      "  File \"/Users/jiro/play/nstrade/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py\", line 1851, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jiro/play/nstrade/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py\", line 1865, in _get_module\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Failed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_tf_utils because of the following error (look up to see its traceback):\n",
      "Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.\n",
      "\n",
      "Training timeseries model TemporalFusionTransformer. \n",
      "\tWindow 0\n",
      "\tbool_features: [], continuous_features: ['close'], skewed_features: ['volume']\n",
      "GluonTS logging is turned on during training. Note that losses reported by GluonTS may not correspond to those specified via `eval_metric`.\n",
      "\tTraining on device 'cpu'\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type                           | Params | Mode  | In sizes                                                                           | Out sizes                    \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "0 | model | TemporalFusionTransformerModel | 126 K  | train | [[1, 64], [1, 64], [1, 1], [1, 1], [1, 65, 4], [1, 65, 0], [1, 64, 2], [1, 64, 0]] | [[[1, 1, 5]], [1, 1], [1, 1]]\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "126 K     Trainable params\n",
      "0         Non-trainable params\n",
      "126 K     Total params\n",
      "0.507     Total estimated model params size (MB)\n",
      "237       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "Epoch 0, global step 50: 'val_loss' reached 0.00339 (best 0.00339), saving model to '/Users/jiro/play/nstrade/notebooks/AutogluonModels/ag-20250517_083050/models/TemporalFusionTransformer/W0/lightning_logs/version_0/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
      "Epoch 1, global step 100: 'val_loss' reached 0.00309 (best 0.00309), saving model to '/Users/jiro/play/nstrade/notebooks/AutogluonModels/ag-20250517_083050/models/TemporalFusionTransformer/W0/lightning_logs/version_0/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
      "Epoch 2, global step 150: 'val_loss' was not in top 1\n",
      "Epoch 3, global step 200: 'val_loss' was not in top 1\n",
      "Epoch 4, global step 250: 'val_loss' was not in top 1\n",
      "Epoch 5, global step 300: 'val_loss' reached 0.00296 (best 0.00296), saving model to '/Users/jiro/play/nstrade/notebooks/AutogluonModels/ag-20250517_083050/models/TemporalFusionTransformer/W0/lightning_logs/version_0/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
      "Epoch 6, global step 350: 'val_loss' was not in top 1\n",
      "Epoch 7, global step 400: 'val_loss' was not in top 1\n",
      "Epoch 8, global step 450: 'val_loss' reached 0.00256 (best 0.00256), saving model to '/Users/jiro/play/nstrade/notebooks/AutogluonModels/ag-20250517_083050/models/TemporalFusionTransformer/W0/lightning_logs/version_0/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
      "Epoch 9, global step 500: 'val_loss' was not in top 1\n",
      "Epoch 10, global step 550: 'val_loss' was not in top 1\n",
      "Epoch 11, global step 600: 'val_loss' was not in top 1\n",
      "Epoch 12, global step 650: 'val_loss' was not in top 1\n",
      "Epoch 13, global step 700: 'val_loss' was not in top 1\n",
      "Epoch 14, global step 750: 'val_loss' was not in top 1\n",
      "Epoch 15, global step 800: 'val_loss' was not in top 1\n",
      "Epoch 16, global step 850: 'val_loss' was not in top 1\n",
      "Epoch 17, global step 900: 'val_loss' was not in top 1\n",
      "Epoch 18, global step 950: 'val_loss' reached 0.00224 (best 0.00224), saving model to '/Users/jiro/play/nstrade/notebooks/AutogluonModels/ag-20250517_083050/models/TemporalFusionTransformer/W0/lightning_logs/version_0/checkpoints/epoch=18-step=950.ckpt' as top 1\n",
      "Epoch 19, global step 1000: 'val_loss' was not in top 1\n",
      "Epoch 20, global step 1050: 'val_loss' was not in top 1\n",
      "Epoch 21, global step 1100: 'val_loss' was not in top 1\n",
      "Epoch 22, global step 1150: 'val_loss' was not in top 1\n",
      "Epoch 23, global step 1200: 'val_loss' was not in top 1\n",
      "Epoch 24, global step 1250: 'val_loss' was not in top 1\n",
      "Epoch 25, global step 1300: 'val_loss' was not in top 1\n",
      "Epoch 26, global step 1350: 'val_loss' was not in top 1\n",
      "Epoch 27, global step 1400: 'val_loss' reached 0.00184 (best 0.00184), saving model to '/Users/jiro/play/nstrade/notebooks/AutogluonModels/ag-20250517_083050/models/TemporalFusionTransformer/W0/lightning_logs/version_0/checkpoints/epoch=27-step=1400.ckpt' as top 1\n",
      "Epoch 28, global step 1450: 'val_loss' was not in top 1\n",
      "Epoch 29, global step 1500: 'val_loss' was not in top 1\n",
      "Epoch 30, global step 1550: 'val_loss' was not in top 1\n",
      "Epoch 31, global step 1600: 'val_loss' was not in top 1\n",
      "Epoch 32, global step 1650: 'val_loss' was not in top 1\n",
      "Epoch 33, global step 1700: 'val_loss' was not in top 1\n",
      "Epoch 34, global step 1750: 'val_loss' was not in top 1\n",
      "Epoch 35, global step 1800: 'val_loss' reached 0.00156 (best 0.00156), saving model to '/Users/jiro/play/nstrade/notebooks/AutogluonModels/ag-20250517_083050/models/TemporalFusionTransformer/W0/lightning_logs/version_0/checkpoints/epoch=35-step=1800.ckpt' as top 1\n",
      "Epoch 36, global step 1850: 'val_loss' was not in top 1\n",
      "Epoch 37, global step 1900: 'val_loss' was not in top 1\n",
      "Epoch 38, global step 1950: 'val_loss' was not in top 1\n",
      "Epoch 39, global step 2000: 'val_loss' was not in top 1\n",
      "Epoch 40, global step 2050: 'val_loss' was not in top 1\n",
      "Epoch 41, global step 2100: 'val_loss' was not in top 1\n",
      "Epoch 42, global step 2150: 'val_loss' was not in top 1\n",
      "Epoch 43, global step 2200: 'val_loss' was not in top 1\n",
      "Epoch 44, global step 2250: 'val_loss' was not in top 1\n",
      "Epoch 45, global step 2300: 'val_loss' was not in top 1\n",
      "Epoch 46, global step 2350: 'val_loss' was not in top 1\n",
      "Epoch 47, global step 2400: 'val_loss' was not in top 1\n",
      "Epoch 48, global step 2450: 'val_loss' was not in top 1\n",
      "Epoch 49, global step 2500: 'val_loss' was not in top 1\n",
      "Epoch 50, global step 2550: 'val_loss' was not in top 1\n",
      "Epoch 51, global step 2600: 'val_loss' was not in top 1\n",
      "Epoch 52, global step 2650: 'val_loss' was not in top 1\n",
      "Epoch 53, global step 2700: 'val_loss' was not in top 1\n",
      "Epoch 54, global step 2750: 'val_loss' was not in top 1\n",
      "Epoch 55, global step 2800: 'val_loss' was not in top 1\n",
      "Removing lightning_logs directory /Users/jiro/play/nstrade/notebooks/AutogluonModels/ag-20250517_083050/models/TemporalFusionTransformer/W0/lightning_logs\n",
      "\t\t-0.0008      = Validation score (-RMSE)\n",
      "\t\t223.610 s    = Training runtime\n",
      "\t\t0.009   s    = Prediction runtime\n",
      "\t-0.0008       = Validation score (-RMSE)\n",
      "\t223.62  s     = Training runtime\n",
      "\t0.01    s     = Validation (prediction) runtime\n",
      "Training timeseries model DeepAR. \n",
      "\tWindow 0\n",
      "GluonTS logging is turned on during training. Note that losses reported by GluonTS may not correspond to those specified via `eval_metric`.\n",
      "\tTraining on device 'cpu'\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type        | Params | Mode  | In sizes                                                     | Out sizes  \n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "0 | model | DeepARModel | 27.6 K | train | [[1, 1], [1, 1], [1, 730, 5], [1, 730], [1, 730], [1, 1, 5]] | [1, 100, 1]\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "27.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "27.6 K    Total params\n",
      "0.111     Total estimated model params size (MB)\n",
      "11        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "Epoch 0, global step 50: 'val_loss' reached -3.03026 (best -3.03026), saving model to '/Users/jiro/play/nstrade/notebooks/AutogluonModels/ag-20250517_083050/models/DeepAR/W0/lightning_logs/version_0/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
      "Epoch 1, global step 100: 'val_loss' reached -3.14093 (best -3.14093), saving model to '/Users/jiro/play/nstrade/notebooks/AutogluonModels/ag-20250517_083050/models/DeepAR/W0/lightning_logs/version_0/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
      "Epoch 2, global step 150: 'val_loss' reached -3.24706 (best -3.24706), saving model to '/Users/jiro/play/nstrade/notebooks/AutogluonModels/ag-20250517_083050/models/DeepAR/W0/lightning_logs/version_0/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
      "Epoch 3, global step 200: 'val_loss' reached -3.27500 (best -3.27500), saving model to '/Users/jiro/play/nstrade/notebooks/AutogluonModels/ag-20250517_083050/models/DeepAR/W0/lightning_logs/version_0/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
      "Epoch 4, global step 250: 'val_loss' reached -3.27712 (best -3.27712), saving model to '/Users/jiro/play/nstrade/notebooks/AutogluonModels/ag-20250517_083050/models/DeepAR/W0/lightning_logs/version_0/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
      "Epoch 5, global step 300: 'val_loss' reached -3.44069 (best -3.44069), saving model to '/Users/jiro/play/nstrade/notebooks/AutogluonModels/ag-20250517_083050/models/DeepAR/W0/lightning_logs/version_0/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
      "Epoch 6, global step 350: 'val_loss' reached -3.52283 (best -3.52283), saving model to '/Users/jiro/play/nstrade/notebooks/AutogluonModels/ag-20250517_083050/models/DeepAR/W0/lightning_logs/version_0/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
      "Epoch 7, global step 400: 'val_loss' reached -3.78114 (best -3.78114), saving model to '/Users/jiro/play/nstrade/notebooks/AutogluonModels/ag-20250517_083050/models/DeepAR/W0/lightning_logs/version_0/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
      "Epoch 8, global step 450: 'val_loss' was not in top 1\n",
      "Epoch 9, global step 500: 'val_loss' reached -3.98060 (best -3.98060), saving model to '/Users/jiro/play/nstrade/notebooks/AutogluonModels/ag-20250517_083050/models/DeepAR/W0/lightning_logs/version_0/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
      "Epoch 10, global step 550: 'val_loss' was not in top 1\n",
      "Epoch 11, global step 600: 'val_loss' reached -4.10243 (best -4.10243), saving model to '/Users/jiro/play/nstrade/notebooks/AutogluonModels/ag-20250517_083050/models/DeepAR/W0/lightning_logs/version_0/checkpoints/epoch=11-step=600.ckpt' as top 1\n",
      "Epoch 12, global step 650: 'val_loss' reached -4.14003 (best -4.14003), saving model to '/Users/jiro/play/nstrade/notebooks/AutogluonModels/ag-20250517_083050/models/DeepAR/W0/lightning_logs/version_0/checkpoints/epoch=12-step=650.ckpt' as top 1\n",
      "Epoch 13, global step 700: 'val_loss' was not in top 1\n",
      "Epoch 14, global step 750: 'val_loss' reached -4.21337 (best -4.21337), saving model to '/Users/jiro/play/nstrade/notebooks/AutogluonModels/ag-20250517_083050/models/DeepAR/W0/lightning_logs/version_0/checkpoints/epoch=14-step=750.ckpt' as top 1\n",
      "Epoch 15, global step 800: 'val_loss' reached -4.38628 (best -4.38628), saving model to '/Users/jiro/play/nstrade/notebooks/AutogluonModels/ag-20250517_083050/models/DeepAR/W0/lightning_logs/version_0/checkpoints/epoch=15-step=800.ckpt' as top 1\n",
      "Epoch 16, global step 850: 'val_loss' was not in top 1\n",
      "Epoch 17, global step 900: 'val_loss' was not in top 1\n",
      "Epoch 18, global step 950: 'val_loss' reached -4.40500 (best -4.40500), saving model to '/Users/jiro/play/nstrade/notebooks/AutogluonModels/ag-20250517_083050/models/DeepAR/W0/lightning_logs/version_0/checkpoints/epoch=18-step=950.ckpt' as top 1\n",
      "Epoch 19, global step 1000: 'val_loss' was not in top 1\n",
      "Epoch 20, global step 1050: 'val_loss' reached -4.42424 (best -4.42424), saving model to '/Users/jiro/play/nstrade/notebooks/AutogluonModels/ag-20250517_083050/models/DeepAR/W0/lightning_logs/version_0/checkpoints/epoch=20-step=1050.ckpt' as top 1\n",
      "Epoch 21, global step 1100: 'val_loss' reached -4.45772 (best -4.45772), saving model to '/Users/jiro/play/nstrade/notebooks/AutogluonModels/ag-20250517_083050/models/DeepAR/W0/lightning_logs/version_0/checkpoints/epoch=21-step=1100.ckpt' as top 1\n",
      "Epoch 22, global step 1150: 'val_loss' reached -4.46809 (best -4.46809), saving model to '/Users/jiro/play/nstrade/notebooks/AutogluonModels/ag-20250517_083050/models/DeepAR/W0/lightning_logs/version_0/checkpoints/epoch=22-step=1150.ckpt' as top 1\n",
      "Epoch 23, global step 1200: 'val_loss' was not in top 1\n",
      "Epoch 24, global step 1250: 'val_loss' reached -4.51508 (best -4.51508), saving model to '/Users/jiro/play/nstrade/notebooks/AutogluonModels/ag-20250517_083050/models/DeepAR/W0/lightning_logs/version_0/checkpoints/epoch=24-step=1250.ckpt' as top 1\n",
      "Epoch 25, global step 1300: 'val_loss' reached -4.59044 (best -4.59044), saving model to '/Users/jiro/play/nstrade/notebooks/AutogluonModels/ag-20250517_083050/models/DeepAR/W0/lightning_logs/version_0/checkpoints/epoch=25-step=1300.ckpt' as top 1\n",
      "Epoch 26, global step 1350: 'val_loss' reached -4.65709 (best -4.65709), saving model to '/Users/jiro/play/nstrade/notebooks/AutogluonModels/ag-20250517_083050/models/DeepAR/W0/lightning_logs/version_0/checkpoints/epoch=26-step=1350.ckpt' as top 1\n",
      "Epoch 27, global step 1400: 'val_loss' was not in top 1\n",
      "Epoch 28, global step 1450: 'val_loss' was not in top 1\n",
      "Epoch 29, global step 1500: 'val_loss' was not in top 1\n",
      "Epoch 30, global step 1550: 'val_loss' was not in top 1\n",
      "Epoch 31, global step 1600: 'val_loss' was not in top 1\n",
      "Epoch 32, global step 1650: 'val_loss' reached -4.72341 (best -4.72341), saving model to '/Users/jiro/play/nstrade/notebooks/AutogluonModels/ag-20250517_083050/models/DeepAR/W0/lightning_logs/version_0/checkpoints/epoch=32-step=1650.ckpt' as top 1\n",
      "Epoch 33, global step 1700: 'val_loss' was not in top 1\n",
      "Epoch 34, global step 1750: 'val_loss' reached -4.78482 (best -4.78482), saving model to '/Users/jiro/play/nstrade/notebooks/AutogluonModels/ag-20250517_083050/models/DeepAR/W0/lightning_logs/version_0/checkpoints/epoch=34-step=1750.ckpt' as top 1\n",
      "Epoch 35, global step 1800: 'val_loss' was not in top 1\n",
      "Epoch 36, global step 1850: 'val_loss' was not in top 1\n",
      "Epoch 37, global step 1900: 'val_loss' was not in top 1\n",
      "Epoch 38, global step 1950: 'val_loss' was not in top 1\n",
      "Epoch 39, global step 2000: 'val_loss' reached -4.79632 (best -4.79632), saving model to '/Users/jiro/play/nstrade/notebooks/AutogluonModels/ag-20250517_083050/models/DeepAR/W0/lightning_logs/version_0/checkpoints/epoch=39-step=2000.ckpt' as top 1\n",
      "Epoch 40, global step 2050: 'val_loss' was not in top 1\n",
      "Epoch 41, global step 2100: 'val_loss' was not in top 1\n",
      "Epoch 42, global step 2150: 'val_loss' was not in top 1\n",
      "Epoch 43, global step 2200: 'val_loss' was not in top 1\n",
      "Epoch 44, global step 2250: 'val_loss' was not in top 1\n",
      "Epoch 45, global step 2300: 'val_loss' was not in top 1\n",
      "Epoch 46, global step 2350: 'val_loss' was not in top 1\n",
      "Epoch 47, global step 2400: 'val_loss' was not in top 1\n",
      "Epoch 48, global step 2450: 'val_loss' was not in top 1\n",
      "Epoch 49, global step 2500: 'val_loss' was not in top 1\n",
      "Epoch 50, global step 2550: 'val_loss' was not in top 1\n",
      "Epoch 51, global step 2600: 'val_loss' was not in top 1\n",
      "Epoch 52, global step 2650: 'val_loss' was not in top 1\n",
      "Epoch 53, global step 2700: 'val_loss' was not in top 1\n",
      "Epoch 54, global step 2750: 'val_loss' was not in top 1\n",
      "Epoch 55, global step 2800: 'val_loss' was not in top 1\n",
      "Epoch 56, global step 2850: 'val_loss' was not in top 1\n",
      "Epoch 57, global step 2900: 'val_loss' was not in top 1\n",
      "Epoch 58, global step 2950: 'val_loss' was not in top 1\n",
      "Epoch 59, global step 3000: 'val_loss' was not in top 1\n",
      "Removing lightning_logs directory /Users/jiro/play/nstrade/notebooks/AutogluonModels/ag-20250517_083050/models/DeepAR/W0/lightning_logs\n",
      "\t\t-0.0011      = Validation score (-RMSE)\n",
      "\t\t22.821  s    = Training runtime\n",
      "\t\t0.007   s    = Prediction runtime\n",
      "\t-0.0011       = Validation score (-RMSE)\n",
      "\t22.83   s     = Training runtime\n",
      "\t0.01    s     = Validation (prediction) runtime\n",
      "Training timeseries model PatchTST. \n",
      "\tWindow 0\n",
      "GluonTS logging is turned on during training. Note that losses reported by GluonTS may not correspond to those specified via `eval_metric`.\n",
      "\tTraining on device 'cpu'\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type          | Params | Mode \n",
      "------------------------------------------------\n",
      "0 | model | PatchTSTModel | 38.9 K | train\n",
      "------------------------------------------------\n",
      "38.5 K    Trainable params\n",
      "384       Non-trainable params\n",
      "38.9 K    Total params\n",
      "0.156     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "Epoch 0, global step 50: 'val_loss' reached -3.56687 (best -3.56687), saving model to '/Users/jiro/play/nstrade/notebooks/AutogluonModels/ag-20250517_083050/models/PatchTST/W0/lightning_logs/version_0/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
      "Epoch 1, global step 100: 'val_loss' reached -3.98862 (best -3.98862), saving model to '/Users/jiro/play/nstrade/notebooks/AutogluonModels/ag-20250517_083050/models/PatchTST/W0/lightning_logs/version_0/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
      "Epoch 2, global step 150: 'val_loss' was not in top 1\n",
      "Epoch 3, global step 200: 'val_loss' was not in top 1\n",
      "Epoch 4, global step 250: 'val_loss' was not in top 1\n",
      "Epoch 5, global step 300: 'val_loss' was not in top 1\n",
      "Epoch 6, global step 350: 'val_loss' was not in top 1\n",
      "Epoch 7, global step 400: 'val_loss' was not in top 1\n",
      "Epoch 8, global step 450: 'val_loss' was not in top 1\n",
      "Epoch 9, global step 500: 'val_loss' was not in top 1\n",
      "Epoch 10, global step 550: 'val_loss' was not in top 1\n",
      "Epoch 11, global step 600: 'val_loss' was not in top 1\n",
      "Epoch 12, global step 650: 'val_loss' was not in top 1\n",
      "Epoch 13, global step 700: 'val_loss' was not in top 1\n",
      "Epoch 14, global step 750: 'val_loss' was not in top 1\n",
      "Epoch 15, global step 800: 'val_loss' was not in top 1\n",
      "Epoch 16, global step 850: 'val_loss' was not in top 1\n",
      "Epoch 17, global step 900: 'val_loss' was not in top 1\n",
      "Epoch 18, global step 950: 'val_loss' was not in top 1\n",
      "Epoch 19, global step 1000: 'val_loss' was not in top 1\n",
      "Epoch 20, global step 1050: 'val_loss' was not in top 1\n",
      "Epoch 21, global step 1100: 'val_loss' was not in top 1\n",
      "Removing lightning_logs directory /Users/jiro/play/nstrade/notebooks/AutogluonModels/ag-20250517_083050/models/PatchTST/W0/lightning_logs\n",
      "\t\t-0.0020      = Validation score (-RMSE)\n",
      "\t\t12.208  s    = Training runtime\n",
      "\t\t0.005   s    = Prediction runtime\n",
      "\t-0.0020       = Validation score (-RMSE)\n",
      "\t12.21   s     = Training runtime\n",
      "\t0.01    s     = Validation (prediction) runtime\n",
      "Training timeseries model TiDE. \n",
      "\tWindow 0\n",
      "GluonTS logging is turned on during training. Note that losses reported by GluonTS may not correspond to those specified via `eval_metric`.\n",
      "\tTraining on device 'cpu'\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type      | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | model | TiDEModel | 655 K  | train\n",
      "--------------------------------------------\n",
      "655 K     Trainable params\n",
      "0         Non-trainable params\n",
      "655 K     Total params\n",
      "2.623     Total estimated model params size (MB)\n",
      "65        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "Epoch 0, global step 100: 'val_loss' reached -3.28296 (best -3.28296), saving model to '/Users/jiro/play/nstrade/notebooks/AutogluonModels/ag-20250517_083050/models/TiDE/W0/lightning_logs/version_0/checkpoints/epoch=0-step=100.ckpt' as top 1\n",
      "Epoch 1, global step 200: 'val_loss' was not in top 1\n",
      "Epoch 2, global step 300: 'val_loss' was not in top 1\n",
      "Epoch 3, global step 400: 'val_loss' was not in top 1\n",
      "Epoch 4, global step 500: 'val_loss' was not in top 1\n",
      "Epoch 5, global step 600: 'val_loss' was not in top 1\n",
      "Epoch 6, global step 700: 'val_loss' was not in top 1\n",
      "Epoch 7, global step 800: 'val_loss' was not in top 1\n",
      "Epoch 8, global step 900: 'val_loss' was not in top 1\n"
     ]
    }
   ],
   "source": [
    "df = process_data_for_model('../data/btc_hour.csv')\n",
    "training_df = df.copy().head(2000)\n",
    "test_df = df.copy().tail(10000)\n",
    "\n",
    "run_backtest(\n",
    "    lambda initial_capital: AutoMLStrategy(initial_capital, training_df),\n",
    "    test_df,\n",
    "    initial_capital=10000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development // Drafts // WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.timeseries import TimeSeriesDataFrame, TimeSeriesPredictor\n",
    "\n",
    "df = df.head(1000)\n",
    "\n",
    "train_df = TimeSeriesDataFrame.from_data_frame(\n",
    "    df,          # drop last NaN caused by shift\n",
    "    timestamp_column='timestamp',\n",
    "    id_column='id',\n",
    "    static_features_df=None\n",
    ")\n",
    "\n",
    "\n",
    "model = TimeSeriesPredictor(\n",
    "    target='return',\n",
    "    prediction_length=1,\n",
    "    eval_metric='rmse',\n",
    "    verbosity=4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_backtest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mrun_backtest\u001b[49m(AutoMLStrategy, df)\n",
      "\u001b[31mNameError\u001b[39m: name 'run_backtest' is not defined"
     ]
    }
   ],
   "source": [
    "run_backtest(AutoMLStrategy, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new strategy\n",
    "class AutoMLStrategy(Strategy):\n",
    "    def __init__(self, initial_capital=10000):\n",
    "        super().__init__(initial_capital)\n",
    "        self.has_bought = False\n",
    "        self.model = model\n",
    "\n",
    "    def process_bar(self, bar):\n",
    "        self.current_bar = bar\n",
    "        self.model.predict(bar)\n",
    "\n",
    "    def get_signal(self):\n",
    "        if not self.has_bought:\n",
    "            self.has_bought = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
